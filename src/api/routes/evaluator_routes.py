# src/api/routes/evaluator_routes.py
from fastapi import APIRouter, UploadFile, File, HTTPException
from pydantic import BaseModel
import sys
import os
import json
from typing import Optional

# Add parent directories to path
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, src_dir)
sys.path.insert(0, os.path.join(src_dir, 'test_evaluator'))

from agents import Runner
from openai.types.responses import ResponseTextDeltaEvent
from test_evaluator.Config.config import config

router = APIRouter()

class EvaluationRequest(BaseModel):
    question: str
    student_answer: str
    subject: Optional[str] = None
    max_marks: Optional[int] = 10

class EvaluationResponse(BaseModel):
    subject: str
    question: str
    student_answer: str
    correct_answer: str
    score: float
    feedback: dict

class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    text: str

@router.post("/evaluate-answer", response_model=EvaluationResponse)
async def evaluate_answer(request: EvaluationRequest):
    """
    Evaluate student's answer and provide feedback
    """
    try:
        # Import evaluator agent
        from test_evaluator.test_eval import test_evaluator
        
        # Create evaluation prompt
        evaluation_prompt = f"""
        Evaluate the following answer:

        Subject: {request.subject or 'General'}
        Question: {request.question}
        Student's Answer: {request.student_answer}
        Maximum Marks: {request.max_marks}

        Please evaluate this answer and provide detailed feedback.
        """
        
        # Collect the full response from streaming
        full_response = ""
        
        agent_response = Runner.run_streamed(
            test_evaluator,
            input=evaluation_prompt,
            run_config=config
        )
        
        async for event in agent_response.stream_events():
            if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                full_response += event.data.delta
        
        # Parse the response (you might need to adjust this based on your agent's output format)
        # For now, returning a structured response
        return EvaluationResponse(
            subject=request.subject or "General",
            question=request.question,
            student_answer=request.student_answer,
            correct_answer="Correct answer will be generated by evaluator",
            score=8.0,  # This should be extracted from the response
            feedback={
                "strengths": "Good understanding of concepts",
                "mistakes": "Minor improvements needed",
                "improvement_tips": "Add more details to your answer"
            }
        )
    
    except Exception as e:
        print(f"Error in evaluation: {str(e)}")
        import traceback
        traceback.print_exc()
        
        raise HTTPException(status_code=500, detail=f"Evaluation error: {str(e)}")

@router.post("/chat", response_model=ChatResponse)
async def chat_with_evaluator(request: ChatRequest):
    """
    Chat with the evaluator agent for general questions
    """
    try:
        from test_evaluator.test_eval import test_evaluator
        
        # Collect the full response from streaming
        full_response = ""
        
        agent_response = Runner.run_streamed(
            test_evaluator,
            input=request.message,
            run_config=config
        )
        
        async for event in agent_response.stream_events():
            if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                full_response += event.data.delta
        
        return ChatResponse(text=full_response)
    
    except Exception as e:
        print(f"Error in evaluator chat: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return ChatResponse(
            text=f"Sorry, I encountered an error: {str(e)}"
        )

@router.post("/evaluate-image")
async def evaluate_image_answer(
    question: str,
    subject: Optional[str] = None,
    max_marks: Optional[int] = 10,
    image: UploadFile = File(...)
):
    """
    Evaluate answer from image (handwritten or typed)
    """
    try:
        # This would require OCR processing
        # For now, returning a placeholder response
        return {
            "message": "Image evaluation feature coming soon",
            "filename": image.filename,
            "question": question,
            "subject": subject
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Image evaluation error: {str(e)}")

@router.get("/subjects")
async def get_available_subjects():
    """
    Get list of available subjects for evaluation
    """
    return {
        "subjects": [
            "Mathematics", "Physics", "Chemistry", "Biology",
            "English", "Urdu", "Islamiat", "Pakistan Studies", 
            "Computer Science", "General"
        ]
    }